{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "import torch\n",
    "# Installation guide for cu11.8 and cu12.1 https://docs.vllm.ai/en/latest/getting_started/installation.html\n",
    "from vllm import LLM\n",
    "from vllm import LLM, SamplingParams\n",
    "from llmformat.llminterface import build_vllm_logits_processor\n",
    "\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def load_model(model_dir, tp_size=1):\n",
    "    llm = LLM(model=model_dir, tensor_parallel_size=tp_size)\n",
    "    return llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    model,\n",
    "    max_new_tokens=100,\n",
    "    user_prompt=None,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    "):\n",
    "    while True:\n",
    "        if user_prompt is None:\n",
    "            user_prompt = input(\"Enter your prompt: \")\n",
    "            \n",
    "        print(f\"User prompt:\\n{user_prompt}\")\n",
    "\n",
    "        print(f\"sampling params: top_p {top_p} and temperature {temperature} for this inference request\")\n",
    "        sampling_param = SamplingParams(top_p=top_p, \n",
    "                                        temperature=temperature, \n",
    "                                        max_tokens=max_new_tokens,\n",
    "                                        logits_processors=[\n",
    "                                            build_vllm_logits_processor(model, \"/root/llmformat/llmformat/json.bnf\")\n",
    "                                        ])\n",
    "\n",
    "        outputs = model.generate(user_prompt, sampling_params=sampling_param)\n",
    "   \n",
    "        print(f\"model output:\\n {user_prompt} {outputs[0].outputs[0].text}\")\n",
    "        user_prompt = input(\"Enter next prompt (press Enter to exit): \")\n",
    "        if not user_prompt:\n",
    "            break\n",
    "\n",
    "def run_script(\n",
    "    model_dir,\n",
    "    tp_size=1,\n",
    "    max_new_tokens=100,\n",
    "    user_prompt=None,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    "):\n",
    "    model = load_model(model_dir, tp_size)\n",
    "    main(model, max_new_tokens, user_prompt, top_p, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-23 20:53:21 llm_engine.py:73] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d330775ef8405fa12ed80dd2c4ca66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e334f23e2346148b73309d0f566be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-23 20:57:36 llm_engine.py:223] # GPU blocks: 726, # CPU blocks: 512\n",
      "INFO 12-23 20:57:37 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-23 20:57:45 model_runner.py:437] Graph capturing finished in 7 secs.\n",
      "User prompt:\n",
      "generate json\n",
      "sampling params: top_p 0.9 and temperature 0.8 for this inference request\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The token list includes unknown token {token_id}. Please check.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig, PeftModel\n\u001b[1;32m      3\u001b[0m local_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/root/model/llama-2-7b/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m run_script(\u001b[39m\"\u001b[39;49m\u001b[39mmeta-llama/Llama-2-7b-hf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mrun_script\u001b[0;34m(model_dir, tp_size, max_new_tokens, user_prompt, top_p, temperature)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_script\u001b[39m(\n\u001b[1;32m     30\u001b[0m     model_dir,\n\u001b[1;32m     31\u001b[0m     tp_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m\n\u001b[1;32m     36\u001b[0m ):\n\u001b[1;32m     37\u001b[0m     model \u001b[39m=\u001b[39m load_model(model_dir, tp_size)\n\u001b[0;32m---> 38\u001b[0m     main(model, max_new_tokens, user_prompt, top_p, temperature)\n",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model, max_new_tokens, user_prompt, top_p, temperature)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msampling params: top_p \u001b[39m\u001b[39m{\u001b[39;00mtop_p\u001b[39m}\u001b[39;00m\u001b[39m and temperature \u001b[39m\u001b[39m{\u001b[39;00mtemperature\u001b[39m}\u001b[39;00m\u001b[39m for this inference request\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m sampling_param \u001b[39m=\u001b[39m SamplingParams(top_p\u001b[39m=\u001b[39mtop_p, \n\u001b[1;32m     16\u001b[0m                                 temperature\u001b[39m=\u001b[39mtemperature, \n\u001b[1;32m     17\u001b[0m                                 max_tokens\u001b[39m=\u001b[39mmax_new_tokens,\n\u001b[1;32m     18\u001b[0m                                 logits_processors\u001b[39m=\u001b[39m[\n\u001b[1;32m     19\u001b[0m                                     build_vllm_logits_processor(model, \u001b[39m\"\u001b[39m\u001b[39m/root/llmformat/llmformat/json.bnf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m                                 ])\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(user_prompt, sampling_params\u001b[39m=\u001b[39;49msampling_param)\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel output:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00muser_prompt\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00moutputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39moutputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m user_prompt \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnter next prompt (press Enter to exit): \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py:165\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm)\u001b[0m\n\u001b[1;32m    162\u001b[0m     token_ids \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m prompt_token_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    163\u001b[0m         i]\n\u001b[1;32m    164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_request(prompt, sampling_params, token_ids)\n\u001b[0;32m--> 165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_engine(use_tqdm)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py:185\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    183\u001b[0m outputs: List[RequestOutput] \u001b[39m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_engine\u001b[39m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 185\u001b[0m     step_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_engine\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    186\u001b[0m     \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m step_outputs:\n\u001b[1;32m    187\u001b[0m         \u001b[39mif\u001b[39;00m output\u001b[39m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:581\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m ignored\n\u001b[1;32m    580\u001b[0m \u001b[39m# Execute the model.\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_workers(\n\u001b[1;32m    582\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mexecute_model\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    583\u001b[0m     seq_group_metadata_list\u001b[39m=\u001b[39;49mseq_group_metadata_list,\n\u001b[1;32m    584\u001b[0m     blocks_to_swap_in\u001b[39m=\u001b[39;49mscheduler_outputs\u001b[39m.\u001b[39;49mblocks_to_swap_in,\n\u001b[1;32m    585\u001b[0m     blocks_to_swap_out\u001b[39m=\u001b[39;49mscheduler_outputs\u001b[39m.\u001b[39;49mblocks_to_swap_out,\n\u001b[1;32m    586\u001b[0m     blocks_to_copy\u001b[39m=\u001b[39;49mscheduler_outputs\u001b[39m.\u001b[39;49mblocks_to_copy,\n\u001b[1;32m    587\u001b[0m )\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_model_outputs(output, scheduler_outputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:755\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, get_all_outputs, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m     work_groups \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers]\n\u001b[1;32m    753\u001b[0m \u001b[39mfor\u001b[39;00m workers \u001b[39min\u001b[39;00m work_groups:\n\u001b[1;32m    754\u001b[0m     all_outputs\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 755\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_workers_in_batch(workers, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    757\u001b[0m \u001b[39mif\u001b[39;00m get_all_outputs:\n\u001b[1;32m    758\u001b[0m     \u001b[39mreturn\u001b[39;00m all_outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:729\u001b[0m, in \u001b[0;36mLLMEngine._run_workers_in_batch\u001b[0;34m(self, workers, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m         executor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(worker, method)\n\u001b[0;32m--> 729\u001b[0m     output \u001b[39m=\u001b[39m executor(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    730\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m    731\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_config\u001b[39m.\u001b[39mworker_use_ray:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/worker/worker.py:159\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m seq_group_metadata_list:\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m {}\n\u001b[0;32m--> 159\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_runner\u001b[39m.\u001b[39;49mexecute_model(seq_group_metadata_list,\n\u001b[1;32m    160\u001b[0m                                          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpu_cache)\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/worker/model_runner.py:354\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    346\u001b[0m hidden_states \u001b[39m=\u001b[39m model_executable(\n\u001b[1;32m    347\u001b[0m     input_ids\u001b[39m=\u001b[39minput_tokens,\n\u001b[1;32m    348\u001b[0m     positions\u001b[39m=\u001b[39minput_positions,\n\u001b[1;32m    349\u001b[0m     kv_caches\u001b[39m=\u001b[39mkv_caches,\n\u001b[1;32m    350\u001b[0m     input_metadata\u001b[39m=\u001b[39minput_metadata,\n\u001b[1;32m    351\u001b[0m )\n\u001b[1;32m    353\u001b[0m \u001b[39m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m    355\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    356\u001b[0m     sampling_metadata\u001b[39m=\u001b[39;49msampling_metadata,\n\u001b[1;32m    357\u001b[0m )\n\u001b[1;32m    358\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:295\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, hidden_states, sampling_metadata)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\n\u001b[1;32m    291\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    292\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    293\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    294\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SamplerOutput:\n\u001b[0;32m--> 295\u001b[0m     next_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampler(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_head\u001b[39m.\u001b[39;49mweight, hidden_states,\n\u001b[1;32m    296\u001b[0m                                sampling_metadata)\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:52\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, embedding, hidden_states, sampling_metadata, embedding_bias)\u001b[0m\n\u001b[1;32m     49\u001b[0m _, vocab_size \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mshape\n\u001b[1;32m     51\u001b[0m \u001b[39m# Apply logits processors (if any).\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m logits \u001b[39m=\u001b[39m _apply_logits_processors(logits, sampling_metadata)\n\u001b[1;32m     54\u001b[0m \u001b[39m# Prepare sampling tensors in another stream to overlap\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m# CPU<->GPU data transfer with GPU computation in forward pass.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mstream(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copy_stream):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:172\u001b[0m, in \u001b[0;36m_apply_logits_processors\u001b[0;34m(logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    170\u001b[0m token_ids \u001b[39m=\u001b[39m sampling_metadata\u001b[39m.\u001b[39mseq_data[seq_id]\u001b[39m.\u001b[39moutput_token_ids\n\u001b[1;32m    171\u001b[0m \u001b[39mfor\u001b[39;00m logits_processor \u001b[39min\u001b[39;00m logits_processors:\n\u001b[0;32m--> 172\u001b[0m     logits_row \u001b[39m=\u001b[39m logits_processor(token_ids, logits_row)\n\u001b[1;32m    173\u001b[0m logits[logits_row_idx] \u001b[39m=\u001b[39m logits_row\n\u001b[1;32m    174\u001b[0m logits_row_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/llmformat-0.0.1.1-py3.10.egg/llmformat/llminterface/vllm.py:22\u001b[0m, in \u001b[0;36mVLLMLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_ids: List[\u001b[39mint\u001b[39m], scores: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     21\u001b[0m     token_sequence \u001b[39m=\u001b[39m input_ids\n\u001b[0;32m---> 22\u001b[0m     allowed_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformatter\u001b[39m.\u001b[39;49mnext_token_from_tokens(token_sequence)\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39mfill_(\u001b[39m-\u001b[39mmath\u001b[39m.\u001b[39minf)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/llmformat-0.0.1.1-py3.10.egg/llmformat/tokenfilter.py:150\u001b[0m, in \u001b[0;36mTokenFilter.next_token_from_tokens\u001b[0;34m(self, prev_token_ids)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m token_id \u001b[39min\u001b[39;00m prev_token_ids:\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m token_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken2symbol:\n\u001b[0;32m--> 150\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mThe token list includes unknown token \u001b[39m\u001b[39m{token_id}\u001b[39;00m\u001b[39m. Please check.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m     symbol_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken2symbol[token_id]\n\u001b[1;32m    152\u001b[0m     \u001b[39mfor\u001b[39;00m symbol \u001b[39min\u001b[39;00m symbol_lst:\n",
      "\u001b[0;31mAssertionError\u001b[0m: The token list includes unknown token {token_id}. Please check."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "local_dir = \"/root/model/llama-2-7b/\"\n",
    "run_script(\"meta-llama/Llama-2-7b-hf\", user_prompt=\"Write a story in json format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
