{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "import torch\n",
    "# Installation guide for cu11.8 and cu12.1 https://docs.vllm.ai/en/latest/getting_started/installation.html\n",
    "from vllm import LLM\n",
    "from vllm import LLM, SamplingParams\n",
    "from llmformat.llminterface import build_vllm_logits_processor\n",
    "\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def load_model(model_dir, tp_size=1):\n",
    "    llm = LLM(model=model_dir, tensor_parallel_size=tp_size)\n",
    "    return llm\n",
    "\n",
    "default_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "def get_prompt(message: str):\n",
    "    return f'<s>[INST] <<SYS>>\\n{default_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    model,\n",
    "    max_new_tokens=100,\n",
    "    user_prompt=None,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    "):\n",
    "    while True:\n",
    "        if user_prompt is None:\n",
    "            user_prompt = input(\"Enter your prompt: \")\n",
    "        \n",
    "        print(f\"User prompt:\\n{user_prompt}\")            \n",
    "        user_prompt = get_prompt(user_prompt)\n",
    "        \n",
    "\n",
    "        print(f\"sampling params: top_p {top_p} and temperature {temperature} for this inference request\")\n",
    "        sampling_param = SamplingParams(top_p=top_p, \n",
    "                                        temperature=temperature, \n",
    "                                        max_tokens=max_new_tokens,\n",
    "                                       )\n",
    "        sampling_param.logits_processors=[\n",
    "                                            #build_vllm_logits_processor(model, \"/root/llmformat/llmformat/json_min.bnf\")\n",
    "                                        ]\n",
    "\n",
    "        outputs = model.generate(user_prompt, sampling_params=sampling_param)\n",
    "   \n",
    "        print(f\"model output:\\n {outputs[0].outputs[0].text}\")\n",
    "        user_prompt = input(\"Enter next prompt (press Enter to exit): \")\n",
    "        if not user_prompt:\n",
    "            break\n",
    "\n",
    "def run_script(\n",
    "    model_dir,\n",
    "    tp_size=1,\n",
    "    max_new_tokens=300,\n",
    "    user_prompt=None,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    "):\n",
    "    model = load_model(model_dir, tp_size)\n",
    "    main(model, max_new_tokens, user_prompt, top_p, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-25 05:40:56 llm_engine.py:73] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 12-25 05:41:01 llm_engine.py:223] # GPU blocks: 726, # CPU blocks: 512\n",
      "INFO 12-25 05:41:02 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-25 05:41:09 model_runner.py:437] Graph capturing finished in 7 secs.\n",
      "User prompt:\n",
      "Represents a=10 b=20 c=30 in json's format.\n",
      "sampling params: top_p 0.9 and temperature 0.8 for this inference request\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output:\n",
      " \n",
      "\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Represents a=10 b=20 c=30 in json's format. [/INST]\n",
      "\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Represents a=\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "run_script(\"meta-llama/Llama-2-7b-hf\", user_prompt=\"Represents a=10 b=20 c=30 in json's format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
