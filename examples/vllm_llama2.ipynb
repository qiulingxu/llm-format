{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "import torch\n",
    "# Installation guide for cu11.8 and cu12.1 https://docs.vllm.ai/en/latest/getting_started/installation.html\n",
    "from vllm import LLM\n",
    "from vllm import LLM, SamplingParams\n",
    "from llmformat.llminterface import build_vllm_logits_processor\n",
    "\n",
    "#torch.cuda.manual_seed(42)\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "def load_model(model_dir, tp_size=1):\n",
    "    llm = LLM(model=model_dir, tensor_parallel_size=tp_size)\n",
    "    return llm\n",
    "\n",
    "default_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "def get_prompt(message: str):\n",
    "    return f'<s>[INST] <<SYS>>\\n{default_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    model,\n",
    "    max_new_tokens=100,\n",
    "    user_prompt=None,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    "):\n",
    "    while True:\n",
    "        if user_prompt is None:\n",
    "            user_prompt = input(\"Enter your prompt: \")\n",
    "        \n",
    "        print(f\"User prompt:\\n{user_prompt}\")            \n",
    "        user_prompt = get_prompt(user_prompt)\n",
    "        \n",
    "\n",
    "        print(f\"sampling params: top_p {top_p} and temperature {temperature} for this inference request\")\n",
    "        sampling_param = SamplingParams(top_p=top_p, \n",
    "                                        temperature=temperature, \n",
    "                                        max_tokens=max_new_tokens,\n",
    "                                       )\n",
    "        sampling_param.logits_processors=[\n",
    "                                            build_vllm_logits_processor(model, \"/root/llmformat/llmformat/json_with_intro.bnf\")\n",
    "                                        ]\n",
    "\n",
    "        outputs = model.generate(user_prompt, sampling_params=sampling_param)\n",
    "   \n",
    "        print(f\"model output:\\n {outputs[0].outputs[0].text}\")\n",
    "        user_prompt = input(\"Enter next prompt (press Enter to exit): \")\n",
    "        if not user_prompt:\n",
    "            break\n",
    "\n",
    "def run_script(\n",
    "    model_dir,\n",
    "    tp_size=1,\n",
    "    max_new_tokens=300,\n",
    "    user_prompt=None,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    "):\n",
    "    model = load_model(model_dir, tp_size)\n",
    "    main(model, max_new_tokens, user_prompt, top_p, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-26 00:14:14 llm_engine.py:73] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-chat-hf', tokenizer='meta-llama/Llama-2-7b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 12-26 00:14:20 llm_engine.py:223] # GPU blocks: 726, # CPU blocks: 512\n",
      "INFO 12-26 00:14:21 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-26 00:14:28 model_runner.py:437] Graph capturing finished in 7 secs.\n",
      "User prompt:\n",
      "Represents a=10 b=20 c=30 in json's format.\n",
      "sampling params: top_p 0.9 and temperature 0.8 for this inference request\n",
      "{'a': 'LA', 'f': 'LF', 's': 'LS', 'l': 'LL', 'e': 'LE', 'n': 'LN', 't': 'LT', 'E': 'UE', 'r': 'LR', 'u': 'LU', '\\\\': 'BACKSLASH', '/': 'FORWARDSLASH', ' ': 'WS', '\\t': 'TAB', '\\n': 'EOL1', '\\r': 'EOL2', '\"': 'DQ', \"'\": 'SQ', ',': 'COMMA', '0': 'DIGIT', '1': 'DIGIT', '2': 'DIGIT', '3': 'DIGIT', '4': 'DIGIT', '5': 'DIGIT', '6': 'DIGIT', '7': 'DIGIT', '8': 'DIGIT', '9': 'DIGIT', '[': 'LSB', ']': 'RSB', '{': 'LCB', '}': 'RCB', ':': 'COLON', '+': 'PLUS', '-': 'MINUS', '.': 'PERIOD', '\\x00': 'OTHER_CHAR', '\\x01': 'OTHER_CHAR', '\\x02': 'OTHER_CHAR', '\\x03': 'OTHER_CHAR', '\\x04': 'OTHER_CHAR', '\\x05': 'OTHER_CHAR', '\\x06': 'OTHER_CHAR', '\\x07': 'OTHER_CHAR', '\\x08': 'OTHER_CHAR', '\\x0b': 'OTHER_CHAR', '\\x0c': 'OTHER_CHAR', '\\x0e': 'OTHER_CHAR', '\\x0f': 'OTHER_CHAR', '\\x10': 'OTHER_CHAR', '\\x11': 'OTHER_CHAR', '\\x12': 'OTHER_CHAR', '\\x13': 'OTHER_CHAR', '\\x14': 'OTHER_CHAR', '\\x15': 'OTHER_CHAR', '\\x16': 'OTHER_CHAR', '\\x17': 'OTHER_CHAR', '\\x18': 'OTHER_CHAR', '\\x19': 'OTHER_CHAR', '\\x1a': 'OTHER_CHAR', '\\x1b': 'OTHER_CHAR', '\\x1c': 'OTHER_CHAR', '\\x1d': 'OTHER_CHAR', '\\x1e': 'OTHER_CHAR', '\\x1f': 'OTHER_CHAR', '!': 'OTHER_CHAR', '#': 'OTHER_CHAR', '$': 'OTHER_CHAR', '%': 'OTHER_CHAR', '&': 'OTHER_CHAR', '(': 'OTHER_CHAR', ')': 'OTHER_CHAR', '*': 'OTHER_CHAR', ';': 'OTHER_CHAR', '<': 'OTHER_CHAR', '=': 'OTHER_CHAR', '>': 'OTHER_CHAR', '?': 'OTHER_CHAR', '@': 'OTHER_CHAR', 'A': 'OTHER_CHAR', 'B': 'OTHER_CHAR', 'C': 'OTHER_CHAR', 'D': 'OTHER_CHAR', 'F': 'OTHER_CHAR', 'G': 'OTHER_CHAR', 'H': 'OTHER_CHAR', 'I': 'OTHER_CHAR', 'J': 'OTHER_CHAR', 'K': 'OTHER_CHAR', 'L': 'OTHER_CHAR', 'M': 'OTHER_CHAR', 'N': 'OTHER_CHAR', 'O': 'OTHER_CHAR', 'P': 'OTHER_CHAR', 'Q': 'OTHER_CHAR', 'R': 'OTHER_CHAR', 'S': 'OTHER_CHAR', 'T': 'OTHER_CHAR', 'U': 'OTHER_CHAR', 'V': 'OTHER_CHAR', 'W': 'OTHER_CHAR', 'X': 'OTHER_CHAR', 'Y': 'OTHER_CHAR', 'Z': 'OTHER_CHAR', '^': 'OTHER_CHAR', '_': 'OTHER_CHAR', '`': 'OTHER_CHAR', 'b': 'OTHER_CHAR', 'c': 'OTHER_CHAR', 'd': 'OTHER_CHAR', 'g': 'OTHER_CHAR', 'h': 'OTHER_CHAR', 'i': 'OTHER_CHAR', 'j': 'OTHER_CHAR', 'k': 'OTHER_CHAR', 'm': 'OTHER_CHAR', 'o': 'OTHER_CHAR', 'p': 'OTHER_CHAR', 'q': 'OTHER_CHAR', 'v': 'OTHER_CHAR', 'w': 'OTHER_CHAR', 'x': 'OTHER_CHAR', 'y': 'OTHER_CHAR', 'z': 'OTHER_CHAR', '|': 'OTHER_CHAR', '~': 'OTHER_CHAR', '\\x7f': 'OTHER_CHAR', '\\x80': 'OTHER_CHAR', '\\x81': 'OTHER_CHAR', '\\x82': 'OTHER_CHAR', '\\x83': 'OTHER_CHAR', '\\x84': 'OTHER_CHAR', '\\x85': 'OTHER_CHAR', '\\x86': 'OTHER_CHAR', '\\x87': 'OTHER_CHAR', '\\x88': 'OTHER_CHAR', '\\x89': 'OTHER_CHAR', '\\x8a': 'OTHER_CHAR', '\\x8b': 'OTHER_CHAR', '\\x8c': 'OTHER_CHAR', '\\x8d': 'OTHER_CHAR', '\\x8e': 'OTHER_CHAR', '\\x8f': 'OTHER_CHAR', '\\x90': 'OTHER_CHAR', '\\x91': 'OTHER_CHAR', '\\x92': 'OTHER_CHAR', '\\x93': 'OTHER_CHAR', '\\x94': 'OTHER_CHAR', '\\x95': 'OTHER_CHAR', '\\x96': 'OTHER_CHAR', '\\x97': 'OTHER_CHAR', '\\x98': 'OTHER_CHAR', '\\x99': 'OTHER_CHAR', '\\x9a': 'OTHER_CHAR', '\\x9b': 'OTHER_CHAR', '\\x9c': 'OTHER_CHAR', '\\x9d': 'OTHER_CHAR', '\\x9e': 'OTHER_CHAR', '\\x9f': 'OTHER_CHAR', '\\xa0': 'OTHER_CHAR', '¡': 'OTHER_CHAR', '¢': 'OTHER_CHAR', '£': 'OTHER_CHAR', '¤': 'OTHER_CHAR', '¥': 'OTHER_CHAR', '¦': 'OTHER_CHAR', '§': 'OTHER_CHAR', '¨': 'OTHER_CHAR', '©': 'OTHER_CHAR', 'ª': 'OTHER_CHAR', '«': 'OTHER_CHAR', '¬': 'OTHER_CHAR', '\\xad': 'OTHER_CHAR', '®': 'OTHER_CHAR', '¯': 'OTHER_CHAR', '°': 'OTHER_CHAR', '±': 'OTHER_CHAR', '²': 'OTHER_CHAR', '³': 'OTHER_CHAR', '´': 'OTHER_CHAR', 'µ': 'OTHER_CHAR', '¶': 'OTHER_CHAR', '·': 'OTHER_CHAR', '¸': 'OTHER_CHAR', '¹': 'OTHER_CHAR', 'º': 'OTHER_CHAR', '»': 'OTHER_CHAR', '¼': 'OTHER_CHAR', '½': 'OTHER_CHAR', '¾': 'OTHER_CHAR', '¿': 'OTHER_CHAR', 'À': 'OTHER_CHAR', 'Á': 'OTHER_CHAR', 'Â': 'OTHER_CHAR', 'Ã': 'OTHER_CHAR', 'Ä': 'OTHER_CHAR', 'Å': 'OTHER_CHAR', 'Æ': 'OTHER_CHAR', 'Ç': 'OTHER_CHAR', 'È': 'OTHER_CHAR', 'É': 'OTHER_CHAR', 'Ê': 'OTHER_CHAR', 'Ë': 'OTHER_CHAR', 'Ì': 'OTHER_CHAR', 'Í': 'OTHER_CHAR', 'Î': 'OTHER_CHAR', 'Ï': 'OTHER_CHAR', 'Ð': 'OTHER_CHAR', 'Ñ': 'OTHER_CHAR', 'Ò': 'OTHER_CHAR', 'Ó': 'OTHER_CHAR', 'Ô': 'OTHER_CHAR', 'Õ': 'OTHER_CHAR', 'Ö': 'OTHER_CHAR', '×': 'OTHER_CHAR', 'Ø': 'OTHER_CHAR', 'Ù': 'OTHER_CHAR', 'Ú': 'OTHER_CHAR', 'Û': 'OTHER_CHAR', 'Ü': 'OTHER_CHAR', 'Ý': 'OTHER_CHAR', 'Þ': 'OTHER_CHAR', 'ß': 'OTHER_CHAR', 'à': 'OTHER_CHAR', 'á': 'OTHER_CHAR', 'â': 'OTHER_CHAR', 'ã': 'OTHER_CHAR', 'ä': 'OTHER_CHAR', 'å': 'OTHER_CHAR', 'æ': 'OTHER_CHAR', 'ç': 'OTHER_CHAR', 'è': 'OTHER_CHAR', 'é': 'OTHER_CHAR', 'ê': 'OTHER_CHAR', 'ë': 'OTHER_CHAR', 'ì': 'OTHER_CHAR', 'í': 'OTHER_CHAR', 'î': 'OTHER_CHAR', 'ï': 'OTHER_CHAR', 'ð': 'OTHER_CHAR', 'ñ': 'OTHER_CHAR', 'ò': 'OTHER_CHAR', 'ó': 'OTHER_CHAR', 'ô': 'OTHER_CHAR', 'õ': 'OTHER_CHAR', 'ö': 'OTHER_CHAR', '÷': 'OTHER_CHAR', 'ø': 'OTHER_CHAR', 'ù': 'OTHER_CHAR', 'ú': 'OTHER_CHAR', 'û': 'OTHER_CHAR', 'ü': 'OTHER_CHAR', 'ý': 'OTHER_CHAR', 'þ': 'OTHER_CHAR', 'ÿ': 'OTHER_CHAR', '<s>': 'WS', '</s>': 'EOS', None: 'WS', '<unk>': 'WS'}\n",
      "In total 18690 nodes Trie created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmodel output:\n",
      "   Hello! I'm here to help you with your question. However, I notice that the format you provided, a=10,b=20,c=30,d=40,e=50,f=60,g=70,h=80,i=90,j=100,k=110,l=120,m=130,n=140,o=150,p=160,q=170,r=180,s=190,t=200,u=210,v=220,w=230,x=240,y=250,z=260,\n",
      "\n",
      "I'm,a=10,b=20,c=30,d=40,e=50,f=60,g=70,h=80,i=90,j=100,k=110,l=120,m=130,n=140,o=150,p=160,q=170,r=180,s=190,t=200,u=210,v=220,\n",
      "User prompt:\n",
      "Write a story in Json's format.\n",
      "sampling params: top_p 0.9 and temperature 0.8 for this inference request\n",
      "{'a': 'LA', 'f': 'LF', 's': 'LS', 'l': 'LL', 'e': 'LE', 'n': 'LN', 't': 'LT', 'E': 'UE', 'r': 'LR', 'u': 'LU', '\\\\': 'BACKSLASH', '/': 'FORWARDSLASH', ' ': 'WS', '\\t': 'TAB', '\\n': 'EOL1', '\\r': 'EOL2', '\"': 'DQ', \"'\": 'SQ', ',': 'COMMA', '0': 'DIGIT', '1': 'DIGIT', '2': 'DIGIT', '3': 'DIGIT', '4': 'DIGIT', '5': 'DIGIT', '6': 'DIGIT', '7': 'DIGIT', '8': 'DIGIT', '9': 'DIGIT', '[': 'LSB', ']': 'RSB', '{': 'LCB', '}': 'RCB', ':': 'COLON', '+': 'PLUS', '-': 'MINUS', '.': 'PERIOD', '\\x00': 'OTHER_CHAR', '\\x01': 'OTHER_CHAR', '\\x02': 'OTHER_CHAR', '\\x03': 'OTHER_CHAR', '\\x04': 'OTHER_CHAR', '\\x05': 'OTHER_CHAR', '\\x06': 'OTHER_CHAR', '\\x07': 'OTHER_CHAR', '\\x08': 'OTHER_CHAR', '\\x0b': 'OTHER_CHAR', '\\x0c': 'OTHER_CHAR', '\\x0e': 'OTHER_CHAR', '\\x0f': 'OTHER_CHAR', '\\x10': 'OTHER_CHAR', '\\x11': 'OTHER_CHAR', '\\x12': 'OTHER_CHAR', '\\x13': 'OTHER_CHAR', '\\x14': 'OTHER_CHAR', '\\x15': 'OTHER_CHAR', '\\x16': 'OTHER_CHAR', '\\x17': 'OTHER_CHAR', '\\x18': 'OTHER_CHAR', '\\x19': 'OTHER_CHAR', '\\x1a': 'OTHER_CHAR', '\\x1b': 'OTHER_CHAR', '\\x1c': 'OTHER_CHAR', '\\x1d': 'OTHER_CHAR', '\\x1e': 'OTHER_CHAR', '\\x1f': 'OTHER_CHAR', '!': 'OTHER_CHAR', '#': 'OTHER_CHAR', '$': 'OTHER_CHAR', '%': 'OTHER_CHAR', '&': 'OTHER_CHAR', '(': 'OTHER_CHAR', ')': 'OTHER_CHAR', '*': 'OTHER_CHAR', ';': 'OTHER_CHAR', '<': 'OTHER_CHAR', '=': 'OTHER_CHAR', '>': 'OTHER_CHAR', '?': 'OTHER_CHAR', '@': 'OTHER_CHAR', 'A': 'OTHER_CHAR', 'B': 'OTHER_CHAR', 'C': 'OTHER_CHAR', 'D': 'OTHER_CHAR', 'F': 'OTHER_CHAR', 'G': 'OTHER_CHAR', 'H': 'OTHER_CHAR', 'I': 'OTHER_CHAR', 'J': 'OTHER_CHAR', 'K': 'OTHER_CHAR', 'L': 'OTHER_CHAR', 'M': 'OTHER_CHAR', 'N': 'OTHER_CHAR', 'O': 'OTHER_CHAR', 'P': 'OTHER_CHAR', 'Q': 'OTHER_CHAR', 'R': 'OTHER_CHAR', 'S': 'OTHER_CHAR', 'T': 'OTHER_CHAR', 'U': 'OTHER_CHAR', 'V': 'OTHER_CHAR', 'W': 'OTHER_CHAR', 'X': 'OTHER_CHAR', 'Y': 'OTHER_CHAR', 'Z': 'OTHER_CHAR', '^': 'OTHER_CHAR', '_': 'OTHER_CHAR', '`': 'OTHER_CHAR', 'b': 'OTHER_CHAR', 'c': 'OTHER_CHAR', 'd': 'OTHER_CHAR', 'g': 'OTHER_CHAR', 'h': 'OTHER_CHAR', 'i': 'OTHER_CHAR', 'j': 'OTHER_CHAR', 'k': 'OTHER_CHAR', 'm': 'OTHER_CHAR', 'o': 'OTHER_CHAR', 'p': 'OTHER_CHAR', 'q': 'OTHER_CHAR', 'v': 'OTHER_CHAR', 'w': 'OTHER_CHAR', 'x': 'OTHER_CHAR', 'y': 'OTHER_CHAR', 'z': 'OTHER_CHAR', '|': 'OTHER_CHAR', '~': 'OTHER_CHAR', '\\x7f': 'OTHER_CHAR', '\\x80': 'OTHER_CHAR', '\\x81': 'OTHER_CHAR', '\\x82': 'OTHER_CHAR', '\\x83': 'OTHER_CHAR', '\\x84': 'OTHER_CHAR', '\\x85': 'OTHER_CHAR', '\\x86': 'OTHER_CHAR', '\\x87': 'OTHER_CHAR', '\\x88': 'OTHER_CHAR', '\\x89': 'OTHER_CHAR', '\\x8a': 'OTHER_CHAR', '\\x8b': 'OTHER_CHAR', '\\x8c': 'OTHER_CHAR', '\\x8d': 'OTHER_CHAR', '\\x8e': 'OTHER_CHAR', '\\x8f': 'OTHER_CHAR', '\\x90': 'OTHER_CHAR', '\\x91': 'OTHER_CHAR', '\\x92': 'OTHER_CHAR', '\\x93': 'OTHER_CHAR', '\\x94': 'OTHER_CHAR', '\\x95': 'OTHER_CHAR', '\\x96': 'OTHER_CHAR', '\\x97': 'OTHER_CHAR', '\\x98': 'OTHER_CHAR', '\\x99': 'OTHER_CHAR', '\\x9a': 'OTHER_CHAR', '\\x9b': 'OTHER_CHAR', '\\x9c': 'OTHER_CHAR', '\\x9d': 'OTHER_CHAR', '\\x9e': 'OTHER_CHAR', '\\x9f': 'OTHER_CHAR', '\\xa0': 'OTHER_CHAR', '¡': 'OTHER_CHAR', '¢': 'OTHER_CHAR', '£': 'OTHER_CHAR', '¤': 'OTHER_CHAR', '¥': 'OTHER_CHAR', '¦': 'OTHER_CHAR', '§': 'OTHER_CHAR', '¨': 'OTHER_CHAR', '©': 'OTHER_CHAR', 'ª': 'OTHER_CHAR', '«': 'OTHER_CHAR', '¬': 'OTHER_CHAR', '\\xad': 'OTHER_CHAR', '®': 'OTHER_CHAR', '¯': 'OTHER_CHAR', '°': 'OTHER_CHAR', '±': 'OTHER_CHAR', '²': 'OTHER_CHAR', '³': 'OTHER_CHAR', '´': 'OTHER_CHAR', 'µ': 'OTHER_CHAR', '¶': 'OTHER_CHAR', '·': 'OTHER_CHAR', '¸': 'OTHER_CHAR', '¹': 'OTHER_CHAR', 'º': 'OTHER_CHAR', '»': 'OTHER_CHAR', '¼': 'OTHER_CHAR', '½': 'OTHER_CHAR', '¾': 'OTHER_CHAR', '¿': 'OTHER_CHAR', 'À': 'OTHER_CHAR', 'Á': 'OTHER_CHAR', 'Â': 'OTHER_CHAR', 'Ã': 'OTHER_CHAR', 'Ä': 'OTHER_CHAR', 'Å': 'OTHER_CHAR', 'Æ': 'OTHER_CHAR', 'Ç': 'OTHER_CHAR', 'È': 'OTHER_CHAR', 'É': 'OTHER_CHAR', 'Ê': 'OTHER_CHAR', 'Ë': 'OTHER_CHAR', 'Ì': 'OTHER_CHAR', 'Í': 'OTHER_CHAR', 'Î': 'OTHER_CHAR', 'Ï': 'OTHER_CHAR', 'Ð': 'OTHER_CHAR', 'Ñ': 'OTHER_CHAR', 'Ò': 'OTHER_CHAR', 'Ó': 'OTHER_CHAR', 'Ô': 'OTHER_CHAR', 'Õ': 'OTHER_CHAR', 'Ö': 'OTHER_CHAR', '×': 'OTHER_CHAR', 'Ø': 'OTHER_CHAR', 'Ù': 'OTHER_CHAR', 'Ú': 'OTHER_CHAR', 'Û': 'OTHER_CHAR', 'Ü': 'OTHER_CHAR', 'Ý': 'OTHER_CHAR', 'Þ': 'OTHER_CHAR', 'ß': 'OTHER_CHAR', 'à': 'OTHER_CHAR', 'á': 'OTHER_CHAR', 'â': 'OTHER_CHAR', 'ã': 'OTHER_CHAR', 'ä': 'OTHER_CHAR', 'å': 'OTHER_CHAR', 'æ': 'OTHER_CHAR', 'ç': 'OTHER_CHAR', 'è': 'OTHER_CHAR', 'é': 'OTHER_CHAR', 'ê': 'OTHER_CHAR', 'ë': 'OTHER_CHAR', 'ì': 'OTHER_CHAR', 'í': 'OTHER_CHAR', 'î': 'OTHER_CHAR', 'ï': 'OTHER_CHAR', 'ð': 'OTHER_CHAR', 'ñ': 'OTHER_CHAR', 'ò': 'OTHER_CHAR', 'ó': 'OTHER_CHAR', 'ô': 'OTHER_CHAR', 'õ': 'OTHER_CHAR', 'ö': 'OTHER_CHAR', '÷': 'OTHER_CHAR', 'ø': 'OTHER_CHAR', 'ù': 'OTHER_CHAR', 'ú': 'OTHER_CHAR', 'û': 'OTHER_CHAR', 'ü': 'OTHER_CHAR', 'ý': 'OTHER_CHAR', 'þ': 'OTHER_CHAR', 'ÿ': 'OTHER_CHAR', '<s>': 'WS', '</s>': 'EOS', None: 'WS', '<unk>': 'WS'}\n",
      "In total 18690 nodes Trie created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output:\n",
      "   Of course! I'm here to help you with any questions you may have. I understand that you want me to provide respectful and helpful responses, and I will do my best to do so. Please feel free to ask me any questions you have,,e.g. \"What is the capital of France?\"\n",
      "\n",
      "I'm \n",
      "\n",
      "I'm \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "run_script(\"meta-llama/Llama-2-7b-chat-hf\", user_prompt=\"Represents a=10 b=20 c=30 in json's format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
