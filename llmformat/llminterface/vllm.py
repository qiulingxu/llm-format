"""Modified based on https://raw.githubusercontent.com/noamgat/lm-format-enforcer/main/lmformatenforcer/integrations/vllm.py
   Thanks @noamgat for the intergration efforts.
"""
try:
    import torch
    import vllm
    from transformers import PreTrainedTokenizerBase
except ImportError:
    raise ImportError('vllm is not installed. Please install it with "pip install vllm"')
from packaging import version

if version.parse(vllm.__version__) < version.parse("0.2.6"):
    raise ImportError('vllm version doesn\'t supports logits control. Please install it with "pip install vllm >= 0.2.6"')
import math
from typing import List, Optional, Union

from llmformat.tokenfilter import TokenFilter


class VLLMLogitsProcessor:
    def __init__(self, formatter: TokenFilter):
        self.formatter = formatter
        self.mask: Optional[torch.Tensor] = None

    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:
        token_sequence = input_ids
        allowed_tokens = self.formatter.next_token_from_tokens(token_sequence)
        #print(allowed_tokens)
        #print(self.formatter.tokenizer.convert_ids_to_tokens(allowed_tokens))
        print("c", end="")
        if self.mask is not None:
            self.mask.fill_(-math.inf)
        else:
            # We create it here because full_like() also copies the device and dtype
            self.mask = torch.full_like(scores, -math.inf)
        self.mask[allowed_tokens] = 0
        scores = scores + self.mask
        return scores


def build_vllm_tokenizer(llm: Union[vllm.LLM, PreTrainedTokenizerBase]) -> PreTrainedTokenizerBase:
    tokenizer = llm.get_tokenizer() if isinstance(llm, vllm.LLM) else llm
    return tokenizer


def build_vllm_logits_processor(llm: Union[vllm.LLM, PreTrainedTokenizerBase], grammar_file:str) -> VLLMLogitsProcessor:
    """Build the logits processor function that llama.cpp will use to filter the tokens generated by the model. The result
    can be passed in the logits_processor list that is sent to the call or generate() method of llama.cpp models."""
    llm = build_vllm_tokenizer(llm)
    
    formatter = TokenFilter(llm, grammar_file)
    return VLLMLogitsProcessor(formatter)


__all__ = ['build_vllm_logits_processor']